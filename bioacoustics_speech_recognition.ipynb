{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3ypK4ZSF9mz"
      },
      "source": [
        "# WebRTC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vcwn0QnvQg6A",
        "outputId": "af257e65-6965-439b-ee0b-075a1d04f4cf"
      },
      "outputs": [],
      "source": [
        "# !apt-get install ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO1HHryQjwjy",
        "outputId": "4e30d4ce-2ae8-416d-f865-2861660feb31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start: 10.41s, End: 14.61s\n",
            "Start: 15.57s, End: 17.31s\n",
            "Start: 19.95s, End: 22.80s\n",
            "Start: 63.36s, End: 64.62s\n",
            "Start: 65.97s, End: 69.69s\n",
            "Start: 70.68s, End: 72.45s\n",
            "Start: 73.11s, End: 75.27s\n",
            "Start: 75.81s, End: 76.80s\n",
            "Start: 80.43s, End: 82.17s\n",
            "Start: 82.32s, End: 83.82s\n",
            "Start: 86.07s, End: 87.09s\n",
            "Start: 88.65s, End: 91.62s\n",
            "Start: 93.45s, End: 95.85s\n",
            "Start: 97.38s, End: 98.94s\n",
            "Start: 100.08s, End: 102.18s\n",
            "Start: 102.51s, End: 103.41s\n",
            "Start: 104.49s, End: 105.12s\n",
            "Start: 114.93s, End: 116.67s\n",
            "Start: 121.05s, End: 121.86s\n",
            "Start: 133.65s, End: 134.61s\n",
            "Start: 137.19s, End: 138.30s\n",
            "Start: 138.48s, End: 139.74s\n",
            "Start: 140.64s, End: 144.33s\n",
            "Start: 145.02s, End: 147.66s\n",
            "Start: 148.23s, End: 150.15s\n",
            "Start: 153.84s, End: 156.57s\n",
            "Start: 157.02s, End: 159.60s\n",
            "Start: 159.87s, End: 160.62s\n",
            "Start: 161.07s, End: 163.05s\n",
            "Start: 163.56s, End: 165.18s\n",
            "Start: 180.66s, End: 181.26s\n",
            "Start: 188.85s, End: 190.74s\n",
            "Start: 191.46s, End: 192.45s\n",
            "Start: 192.90s, End: 196.86s\n",
            "Start: 201.63s, End: 204.45s\n",
            "Start: 204.45s, End: 205.41s\n",
            "Start: 216.24s, End: 218.64s\n",
            "Start: 219.96s, End: 220.98s\n",
            "Start: 221.16s, End: 222.45s\n",
            "Start: 223.86s, End: 225.60s\n",
            "Start: 226.86s, End: 229.38s\n",
            "Start: 229.80s, End: 232.95s\n",
            "Start: 233.82s, End: 236.67s\n",
            "Start: 236.88s, End: 238.44s\n",
            "Start: 238.86s, End: 240.18s\n",
            "Start: 240.48s, End: 242.04s\n",
            "Start: 258.39s, End: 262.20s\n",
            "Start: 263.04s, End: 264.06s\n",
            "Start: 265.47s, End: 268.26s\n",
            "Start: 270.90s, End: 272.34s\n",
            "Start: 273.18s, End: 274.53s\n",
            "Start: 274.68s, End: 275.61s\n",
            "Start: 276.57s, End: 277.92s\n",
            "Start: 281.40s, End: 283.83s\n",
            "Start: 285.57s, End: 286.50s\n",
            "Start: 287.19s, End: 289.74s\n",
            "Start: 296.34s, End: 301.29s\n",
            "Start: 301.95s, End: 302.88s\n",
            "Start: 307.77s, End: 310.02s\n",
            "Start: 314.25s, End: 318.87s\n",
            "Start: 319.62s, End: 320.70s\n",
            "Start: 320.76s, End: 325.38s\n",
            "Start: 326.37s, End: 327.84s\n",
            "Start: 346.44s, End: 348.30s\n",
            "Start: 348.60s, End: 351.03s\n",
            "Start: 353.16s, End: 355.26s\n",
            "Start: 382.41s, End: 383.94s\n",
            "Start: 384.09s, End: 385.71s\n",
            "Start: 386.97s, End: 388.41s\n",
            "Start: 389.49s, End: 391.89s\n",
            "Start: 392.22s, End: 393.24s\n",
            "Start: 405.03s, End: 405.99s\n",
            "Start: 419.88s, End: 422.88s\n",
            "Start: 423.30s, End: 424.56s\n",
            "Start: 424.74s, End: 425.82s\n",
            "Start: 427.20s, End: 430.29s\n",
            "Start: 430.50s, End: 432.09s\n",
            "Start: 446.88s, End: 447.54s\n",
            "Start: 451.26s, End: 452.58s\n",
            "Start: 468.66s, End: 469.41s\n",
            "Start: 473.25s, End: 474.27s\n",
            "Start: 484.23s, End: 485.04s\n",
            "Start: 487.89s, End: 488.67s\n",
            "Start: 500.37s, End: 502.14s\n",
            "Start: 506.52s, End: 507.51s\n",
            "Start: 520.98s, End: 522.42s\n",
            "Start: 522.69s, End: 524.64s\n",
            "Start: 556.71s, End: 557.58s\n",
            "Start: 570.87s, End: 571.71s\n",
            "Start: 585.24s, End: 585.84s\n",
            "Start: 589.98s, End: 590.58s\n",
            "Start: 593.52s, End: 594.45s\n",
            "Start: 596.07s, End: 597.03s\n",
            "Start: 599.76s, End: 600.66s\n",
            "Start: 602.76s, End: 603.69s\n",
            "Start: 606.36s, End: 606.96s\n",
            "Start: 610.59s, End: 611.85s\n",
            "Start: 622.11s, End: 625.65s\n",
            "Start: 627.03s, End: 627.75s\n",
            "Start: 627.75s, End: 629.46s\n",
            "Start: 630.39s, End: 630.99s\n",
            "Start: 633.21s, End: 635.28s\n",
            "Start: 671.70s, End: 674.64s\n",
            "Start: 700.08s, End: 704.91s\n",
            "Start: 742.68s, End: 745.98s\n",
            "Start: 750.87s, End: 753.54s\n",
            "Start: 754.59s, End: 757.35s\n",
            "Start: 758.31s, End: 762.03s\n",
            "Start: 762.54s, End: 764.19s\n",
            "Start: 764.79s, End: 766.38s\n",
            "Start: 766.50s, End: 768.63s\n",
            "Start: 768.63s, End: 773.25s\n",
            "Start: 774.00s, End: 778.53s\n",
            "Start: 779.31s, End: 780.66s\n",
            "Start: 780.99s, End: 782.10s\n",
            "Start: 782.37s, End: 785.46s\n",
            "Start: 791.58s, End: 796.20s\n"
          ]
        }
      ],
      "source": [
        "import webrtcvad\n",
        "import collections\n",
        "import contextlib\n",
        "import wave\n",
        "import os\n",
        "import subprocess\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# Convert MP3 to WAV\n",
        "def convert_mp3_to_wav(mp3_path, wav_path):\n",
        "    audio = AudioSegment.from_mp3(mp3_path)\n",
        "    audio = audio.set_frame_rate(16000)  # Resample to 16000 Hz\n",
        "    audio = audio.set_channels(1)  # Ensure audio is mono\n",
        "    audio.export(wav_path, format=\"wav\")\n",
        "\n",
        "# Frame class to hold audio data\n",
        "class Frame(object):\n",
        "    def __init__(self, bytes, timestamp, duration):\n",
        "        self.bytes = bytes\n",
        "        self.timestamp = timestamp\n",
        "        self.duration = duration\n",
        "\n",
        "def read_wave(path):\n",
        "    with contextlib.closing(wave.open(path, 'rb')) as wf:\n",
        "        num_channels = wf.getnchannels()\n",
        "        assert num_channels == 1\n",
        "        sample_width = wf.getsampwidth()\n",
        "        assert sample_width == 2\n",
        "        sample_rate = wf.getframerate()\n",
        "        assert sample_rate in (8000, 16000, 32000, 48000)\n",
        "        pcm_data = wf.readframes(wf.getnframes())\n",
        "        return pcm_data, sample_rate\n",
        "\n",
        "def frame_generator(frame_duration_ms, audio, sample_rate):\n",
        "    n = int(sample_rate * (frame_duration_ms / 1000.0) * 2)\n",
        "    offset = 0\n",
        "    timestamp = 0.0\n",
        "    duration = (float(n) / sample_rate) / 2.0\n",
        "    while offset + n < len(audio):\n",
        "        yield Frame(audio[offset:offset + n], timestamp, duration)\n",
        "        timestamp += duration\n",
        "        offset += n\n",
        "\n",
        "def vad_collector(sample_rate, frame_duration_ms, padding_duration_ms, vad, frames):\n",
        "    num_padding_frames = int(padding_duration_ms / frame_duration_ms)\n",
        "    ring_buffer = collections.deque(maxlen=num_padding_frames)\n",
        "    triggered = False\n",
        "\n",
        "    voiced_frames = []\n",
        "    intervals = []\n",
        "\n",
        "    for frame in frames:\n",
        "        is_speech = vad.is_speech(frame.bytes, sample_rate)\n",
        "\n",
        "        if not triggered:\n",
        "            ring_buffer.append((frame, is_speech))\n",
        "            num_voiced = len([f for f, speech in ring_buffer if speech])\n",
        "            if num_voiced > 0.9 * ring_buffer.maxlen:\n",
        "                triggered = True\n",
        "                intervals.append((ring_buffer[0][0].timestamp, None))\n",
        "                for f, s in ring_buffer:\n",
        "                    voiced_frames.append(f)\n",
        "                ring_buffer.clear()\n",
        "        else:\n",
        "            voiced_frames.append(frame)\n",
        "            ring_buffer.append((frame, is_speech))\n",
        "            num_unvoiced = len([f for f, speech in ring_buffer if not speech])\n",
        "            if num_unvoiced > 0.9 * ring_buffer.maxlen:\n",
        "                intervals[-1] = (intervals[-1][0], frame.timestamp + frame.duration)\n",
        "                triggered = False\n",
        "                ring_buffer.clear()\n",
        "\n",
        "    if triggered:\n",
        "        intervals[-1] = (intervals[-1][0], frame.timestamp + frame.duration)\n",
        "\n",
        "    return intervals\n",
        "\n",
        "def save_audio_with_intervals(wav_path, intervals, output_with_voice, output_with_silence):\n",
        "    audio = AudioSegment.from_wav(wav_path)\n",
        "    silent_audio = AudioSegment.silent(duration=len(audio))\n",
        "\n",
        "    audio_with_voice = silent_audio\n",
        "    audio_with_silence = audio\n",
        "\n",
        "    for start, end in intervals:\n",
        "        start_ms = int(start * 1000)\n",
        "        end_ms = int(end * 1000)\n",
        "        audio_with_voice = audio_with_voice.overlay(audio[start_ms:end_ms], position=start_ms)\n",
        "        audio_with_silence = audio_with_silence.overlay(silent_audio[start_ms:end_ms], position=start_ms)\n",
        "\n",
        "    audio_with_voice.export(output_with_voice, format=\"wav\")\n",
        "    audio_with_silence.export(output_with_silence, format=\"wav\")\n",
        "\n",
        "# Paths\n",
        "mp3_path = os.path.join(\"data\",\"XC240120 - Soundscape.mp3\")  # Upload your MP3 file to Colab\n",
        "wav_path = \"temp_audio.wav\"\n",
        "output_with_voice = \"output_with_voice_WebRTC.wav\"\n",
        "output_with_silence = \"output_with_silence_WebRTC.wav\"\n",
        "\n",
        "# Convert MP3 to WAV\n",
        "convert_mp3_to_wav(mp3_path, wav_path)\n",
        "\n",
        "# Read WAV file\n",
        "audio, sample_rate = read_wave(wav_path)\n",
        "\n",
        "# Initialize VAD\n",
        "vad = webrtcvad.Vad(3)\n",
        "\n",
        "# Generate frames\n",
        "frames = frame_generator(30, audio, sample_rate)\n",
        "frames = list(frames)\n",
        "\n",
        "# Collect voiced segments\n",
        "intervals = vad_collector(sample_rate, 30, 300, vad, frames)\n",
        "\n",
        "# Print intervals\n",
        "for start, end in intervals:\n",
        "    print(f\"Start: {start:.2f}s, End: {end:.2f}s\")\n",
        "\n",
        "# Save new audio files with intervals\n",
        "save_audio_with_intervals(wav_path, intervals, output_with_voice, output_with_silence)\n",
        "\n",
        "# Cleanup temporary WAV file\n",
        "os.remove(wav_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kk6HOJilGDe7"
      },
      "source": [
        "# Hugging Face Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "pBkwALjQRCJJ",
        "outputId": "9497ce51-0f6c-4208-a98a-dcbe0aff4f84"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\USER\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "c:\\Users\\USER\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\pyannote\\audio\\core\\io.py:47: UserWarning: \n",
            "torchcodec is not installed correctly so built-in audio decoding will fail. Solutions are:\n",
            "* use audio preloaded in-memory as a {'waveform': (channel, time) torch.Tensor, 'sample_rate': int} dictionary;\n",
            "* fix torchcodec installation. Error message was:\n",
            "\n",
            "Could not load libtorchcodec. Likely causes:\n",
            "          1. FFmpeg is not properly installed in your environment. We support\n",
            "             versions 4, 5, 6 and 7.\n",
            "          2. The PyTorch version (2.8.0+cpu) is not compatible with\n",
            "             this version of TorchCodec. Refer to the version compatibility\n",
            "             table:\n",
            "             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.\n",
            "          3. Another runtime dependency; see exceptions below.\n",
            "        The following exceptions were raised as we tried to load libtorchcodec:\n",
            "        \n",
            "[start of libtorchcodec loading traceback]\n",
            "FFmpeg version 7: Could not find module 'C:\\Users\\USER\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\torchcodec\\libtorchcodec_core7.dll' (or one of its dependencies). Try using the full path with constructor syntax.\n",
            "FFmpeg version 6: Could not find module 'C:\\Users\\USER\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\torchcodec\\libtorchcodec_core6.dll' (or one of its dependencies). Try using the full path with constructor syntax.\n",
            "FFmpeg version 5: Could not find module 'C:\\Users\\USER\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\torchcodec\\libtorchcodec_core5.dll' (or one of its dependencies). Try using the full path with constructor syntax.\n",
            "FFmpeg version 4: Could not find module 'C:\\Users\\USER\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\torchcodec\\libtorchcodec_core4.dll' (or one of its dependencies). Try using the full path with constructor syntax.\n",
            "[end of libtorchcodec loading traceback].\n",
            "  warnings.warn(\n",
            "c:\\Users\\USER\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\USER\\.cache\\huggingface\\hub\\models--pyannote--voice-activity-detection. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Revisions must be passed with `revision` keyword argument.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     18\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mpyannote/voice-activity-detection\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Or the specific model you want\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m#tokenizer = AutoTokenizer.from_pretrained(model_name) #The model does not have a tokenizer\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m#model = AutoModelForSequenceClassification.from_pretrained(model_name) #This model is not a sequence classification\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m#vad = pipeline(\"audio-classification\", model=model, tokenizer=tokenizer) #There is not audio-classification task\u001b[39;00m\n\u001b[32m     22\u001b[39m \n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m#Load pyannote pipeline\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m pipeline = \u001b[43mPipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Function to load and preprocess audio\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_and_preprocess_audio\u001b[39m(audio_path):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\pyannote\\audio\\core\\pipeline.py:244\u001b[39m, in \u001b[36mPipeline.from_pretrained\u001b[39m\u001b[34m(cls, checkpoint, revision, hparams_file, token, cache_dir)\u001b[39m\n\u001b[32m    242\u001b[39m params.setdefault(\u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m, token)\n\u001b[32m    243\u001b[39m params.setdefault(\u001b[33m\"\u001b[39m\u001b[33mcache_dir\u001b[39m\u001b[33m\"\u001b[39m, cache_dir)\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m pipeline = \u001b[43mKlass\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[38;5;66;03m# save pipeline origin (HF, local, etc) and class name as attributes for telemetry purposes\u001b[39;00m\n\u001b[32m    247\u001b[39m pipeline._otel_origin = otel_origin\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\pyannote\\audio\\pipelines\\voice_activity_detection.py:109\u001b[39m, in \u001b[36mVoiceActivityDetection.__init__\u001b[39m\u001b[34m(self, segmentation, fscore, token, cache_dir, **inference_kwargs)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28mself\u001b[39m.fscore = fscore\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# load model and send it to GPU (when available and not already on GPU)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m model = \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegmentation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m inference_kwargs[\u001b[33m\"\u001b[39m\u001b[33mpre_aggregation_hook\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mlambda\u001b[39;00m scores: np.max(\n\u001b[32m    112\u001b[39m     scores, axis=-\u001b[32m1\u001b[39m, keepdims=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    113\u001b[39m )\n\u001b[32m    114\u001b[39m \u001b[38;5;28mself\u001b[39m._segmentation = Inference(model, **inference_kwargs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\pyannote\\audio\\pipelines\\utils\\getter.py:109\u001b[39m, in \u001b[36mget_model\u001b[39m\u001b[34m(model, token, cache_dir)\u001b[39m\n\u001b[32m    106\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     _model = \u001b[43mModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _model:\n\u001b[32m    116\u001b[39m         model = _model\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\pyannote\\audio\\core\\model.py:573\u001b[39m, in \u001b[36mModel.from_pretrained\u001b[39m\u001b[34m(cls, checkpoint, map_location, strict, subfolder, revision, token, cache_dir, **kwargs)\u001b[39m\n\u001b[32m    571\u001b[39m checkpoint = \u001b[38;5;28mstr\u001b[39m(checkpoint)\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m@\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m checkpoint:\n\u001b[32m--> \u001b[39m\u001b[32m573\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    574\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRevisions must be passed with `revision` keyword argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    575\u001b[39m     )\n\u001b[32m    577\u001b[39m path_to_model_checkpoint = download_from_hf_hub(\n\u001b[32m    578\u001b[39m     checkpoint,\n\u001b[32m    579\u001b[39m     AssetFileName.Model,\n\u001b[32m   (...)\u001b[39m\u001b[32m    583\u001b[39m     token=token,\n\u001b[32m    584\u001b[39m )\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m path_to_model_checkpoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[31mValueError\u001b[39m: Revisions must be passed with `revision` keyword argument."
          ]
        }
      ],
      "source": [
        "from pyannote.audio import Pipeline\n",
        "from pydub import AudioSegment\n",
        "import numpy as np\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from transformers import pipeline as hf_pipeline\n",
        "\n",
        "# Read your Hugging Face token from environment. Do NOT hardcode secrets.\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\") or os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
        "if not HF_TOKEN:\n",
        "    raise RuntimeError(\n",
        "        \"Missing Hugging Face token. Set HF_TOKEN or HUGGINGFACE_HUB_TOKEN in your environment.\"\n",
        "    )\n",
        "\n",
        "# Optionally ensure the token is active for the session (no-op if already logged in)\n",
        "try:\n",
        "    login(HF_TOKEN, add_to_git_credential=False)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Model identifier for VAD (pyannote 3.1-compatible)\n",
        "model_name = \"pyannote/voice-activity-detection-3.1\"\n",
        "\n",
        "# Load pyannote pipeline with token (pyannote 3.x uses `token`)\n",
        "pipeline = Pipeline.from_pretrained(model_name, token=HF_TOKEN)\n",
        "\n",
        "# Function to load and preprocess audio\n",
        "def load_and_preprocess_audio(audio_path):\n",
        "    \"\"\"Loads and preprocesses audio for VAD.  Resamples to 16kHz mono, converts to NumPy array.\"\"\"\n",
        "    try:\n",
        "        audio = AudioSegment.from_file(audio_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading audio file {audio_path}: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "    audio = audio.set_channels(1)  # Convert to mono\n",
        "    audio = audio.set_frame_rate(16000)  # Resample to 16kHz\n",
        "    samples = np.array(audio.get_array_of_samples())\n",
        "    samples = samples.astype(np.float32) / np.iinfo(np.int16).max  # Normalize\n",
        "    return samples, audio, audio.frame_rate\n",
        "\n",
        "# Function to perform VAD\n",
        "def detect_speech_intervals(samples, sample_rate, pipeline):\n",
        "    \"\"\"Performs voice activity detection on the audio.\"\"\"\n",
        "\n",
        "    try:\n",
        "        # The pipeline expects a dictionary with 'waveform' and 'sample_rate'\n",
        "        input_data = {\"waveform\": torch.from_numpy(samples).unsqueeze(0), \"sample_rate\": sample_rate}\n",
        "        vad_result = pipeline(input_data)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during VAD: {e}\")\n",
        "        return [], 0.0\n",
        "\n",
        "    intervals = []\n",
        "    for segment in vad_result.get_timeline():\n",
        "        intervals.append((segment.start, segment.end))\n",
        "    return intervals, 0.0\n",
        "\n",
        "# Function to save audio with intervals\n",
        "def save_audio_with_intervals(audio, intervals, output_with_voice, output_with_silence, duration):\n",
        "    silent_audio = AudioSegment.silent(duration=len(audio))\n",
        "\n",
        "    audio_with_voice = silent_audio\n",
        "    audio_with_silence = audio\n",
        "\n",
        "    for start, end in intervals:\n",
        "        start_ms = int(start * 1000)\n",
        "        end_ms = int(end * 1000)\n",
        "        audio_with_voice = audio_with_voice.overlay(audio[start_ms:end_ms], position=start_ms)\n",
        "        audio_with_silence = audio_with_silence.overlay(silent_audio[start_ms:end_ms], position=start_ms)\n",
        "\n",
        "    audio_with_voice.export(output_with_voice, format=\"wav\")\n",
        "    audio_with_silence.export(output_with_silence, format=\"wav\")\n",
        "\n",
        "# Paths\n",
        "# Update this to a valid local path on your machine\n",
        "mp3_path = os.path.join(\"data\",\"XC240120 - Soundscape.mp3\") \n",
        "\n",
        "# Validate input path early\n",
        "if not os.path.isfile(mp3_path):\n",
        "    raise FileNotFoundError(\n",
        "        f\"Audio file not found: {mp3_path}. Update 'mp3_path' to a valid local file.\"\n",
        "    )\n",
        "output_with_voice = \"output_with_voice_huggingface.wav\"\n",
        "output_with_silence = \"output_with_silence_huggingface.wav\"\n",
        "\n",
        "# Load and preprocess audio\n",
        "samples, audio_segment, sample_rate = load_and_preprocess_audio(mp3_path)\n",
        "\n",
        "# Detect speech intervals\n",
        "speech_intervals, duration = detect_speech_intervals(samples, sample_rate, pipeline)\n",
        "duration = len(audio_segment) / 1000.0\n",
        "\n",
        "print(speech_intervals)\n",
        "\n",
        "# Save new audio files with intervals\n",
        "save_audio_with_intervals(audio_segment, speech_intervals, output_with_voice, output_with_silence, duration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1L41vL6Q6KC"
      },
      "source": [
        "## Old version performance evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItX6Em9hDCQc"
      },
      "outputs": [],
      "source": [
        "from pyannote.audio import Pipeline\n",
        "from pydub import AudioSegment\n",
        "import numpy as np\n",
        "import os  # For file existence check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KULDOHjkLxMz",
        "outputId": "e70bd69b-38f2-40b2-bb78-28e9edfdf8a6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/utilities/migration/migration.py:208: You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys that would end up colliding with each other after an upgrade, which means we can't differentiate which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint` callbacks will not be able to reload the state.\n",
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.1.3 to v2.5.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/pyannote/models--pyannote--segmentation/snapshots/059e96f964841d40f1a5e755bb7223f76666bba4/pytorch_model.bin`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.7.1, yours is 2.5.1+cu124. Bad things might happen unless you revert torch to 1.x.\n",
            "VAD Intervals (Pipeline): [(1.16159375, 4.840343750000001), (5.24534375, 9.329093750000002), (10.52721875, 15.809093750000002), (17.395343750000002, 20.669093750000002), (21.17534375, 23.21721875), (24.48284375, 28.515968750000003), (29.309093750000002, 37.25721875), (39.36659375, 41.03721875), (42.06659375, 45.188468750000006), (47.98971875, 48.985343750000006), (50.70659375, 52.73159375), (166.84034375000002, 168.86534375000002), (238.01909375000002, 241.00596875000002), (256.56471875, 259.50096875), (259.72034375000004, 262.74096875000004), (265.25534375, 265.72784375000003), (270.11534375, 272.20784375), (272.41034375000004, 282.34971875), (494.33346875, 495.46409375), (523.71284375, 526.76721875), (527.8640937499999, 529.9734687499999), (530.44596875, 530.8003437500001), (531.6440937500001, 532.85909375)]\n",
            "Manual Intervals: [(0.0, 52.929649), (168.726549, 170.620447), (239.7336, 242.758184), (258.248576, 284.254343), (495.692538, 497.275498), (525.768773, 535.040394)]\n",
            "VAD intervals and performance metrics saved to Labels_XC237810_1.txt\n",
            "Precision: 0.8782\n",
            "Recall: 0.6442\n",
            "F1-score: 0.7432\n",
            "Finished!\n"
          ]
        }
      ],
      "source": [
        "# Load the pre-trained VAD pipeline\n",
        "try:\n",
        "    pipeline = Pipeline.from_pretrained(\"pyannote/voice-activity-detection\", use_auth_token=True)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading pipeline: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Function to load audio\n",
        "def load_audio(audio_path):\n",
        "    audio = AudioSegment.from_file(audio_path)\n",
        "    return audio\n",
        "\n",
        "# Function to perform VAD\n",
        "def detect_speech_intervals(audio_path, pipeline):\n",
        "    audio = load_audio(audio_path)\n",
        "    duration = len(audio) / 1000.0  # Convert to seconds\n",
        "    vad_result = pipeline({\"uri\": audio_path, \"audio\": audio_path})\n",
        "\n",
        "    intervals = []\n",
        "    for segment in vad_result.get_timeline():\n",
        "        intervals.append((segment.start, segment.end))\n",
        "    return intervals, duration\n",
        "\n",
        "# Function to load manual labels from file\n",
        "def load_manual_labels(label_file):\n",
        "    \"\"\"Loads manual labels from the given text file.\"\"\"\n",
        "    labels = []\n",
        "    try:\n",
        "        with open(label_file, 'r') as f:\n",
        "            for line in f:\n",
        "                start, end, label = line.strip().split('\\t')\n",
        "                labels.append((float(start), float(end)))\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Label file not found: {label_file}\")\n",
        "        return None\n",
        "    except ValueError:\n",
        "        print(f\"Error: Invalid format in label file {label_file}.  Expecting tab-separated start, end, label.\")\n",
        "        return None\n",
        "    return labels\n",
        "\n",
        "# Function to calculate precision and recall\n",
        "def calculate_precision_recall_f1(vad_intervals, manual_intervals):\n",
        "    \"\"\"Calculates precision, recall, and F1-score.\"\"\"\n",
        "\n",
        "    # Convert intervals to sets of time points for easier comparison\n",
        "    def intervals_to_set(intervals):\n",
        "        time_points = set()\n",
        "        for start, end in intervals:\n",
        "            for i in range(int(start * 100), int(end * 100)):  # Convert to 10ms resolution\n",
        "                time_points.add(i)\n",
        "        return time_points\n",
        "\n",
        "    vad_set = intervals_to_set(vad_intervals)\n",
        "    manual_set = intervals_to_set(manual_intervals)\n",
        "\n",
        "    true_positives = len(vad_set.intersection(manual_set))\n",
        "    predicted_positives = len(vad_set)\n",
        "    actual_positives = len(manual_set)\n",
        "\n",
        "    precision = true_positives / predicted_positives if predicted_positives > 0 else 0\n",
        "    recall = true_positives / actual_positives if actual_positives > 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    return precision, recall, f1\n",
        "\n",
        "# Function to save VAD intervals to a file, including performance metrics\n",
        "def save_vad_intervals(vad_intervals, manual_intervals, output_file):\n",
        "    \"\"\"Saves VAD intervals to a text file, including performance metrics.\"\"\"\n",
        "    try:\n",
        "        with open(output_file, 'w') as f:\n",
        "            # Calculate performance metrics\n",
        "            precision, recall, f1 = calculate_precision_recall_f1(vad_intervals, manual_intervals)\n",
        "\n",
        "            # Write header with performance metrics\n",
        "            f.write(f\"# Precision: {precision:.4f}\\n\")\n",
        "            f.write(f\"# Recall: {recall:.4f}\\n\")\n",
        "            f.write(f\"# F1-score: {f1:.4f}\\n\")\n",
        "            f.write(\"# start\\tend\\tVAD\\n\") # Header for intervals\n",
        "\n",
        "            # Write VAD intervals\n",
        "            for start, end in vad_intervals:\n",
        "                f.write(f\"{start}\\t{end}\\tVAD\\n\")\n",
        "\n",
        "        print(f\"VAD intervals and performance metrics saved to {output_file}\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall: {recall:.4f}\")\n",
        "        print(f\"F1-score: {f1:.4f}\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving VAD intervals: {e}\")\n",
        "\n",
        "# Function to save audio with intervals\n",
        "def save_audio_with_intervals(audio, intervals, output_with_voice, output_with_silence, duration):\n",
        "    silent_audio = AudioSegment.silent(duration=len(audio))\n",
        "\n",
        "    audio_with_voice = silent_audio\n",
        "    audio_with_silence = audio\n",
        "\n",
        "    for start, end in intervals:\n",
        "        start_ms = int(start * 1000)\n",
        "        end_ms = int(end * 1000)\n",
        "        audio_with_voice = audio_with_voice.overlay(audio[start_ms:end_ms], position=start_ms)\n",
        "        audio_with_silence = audio_with_silence.overlay(silent_audio[start_ms:end_ms], position=start_ms)\n",
        "\n",
        "    audio_with_voice.export(output_with_voice, format=\"wav\")\n",
        "    audio_with_silence.export(output_with_silence, format=\"wav\")\n",
        "\n",
        "# Paths\n",
        "mp3_path = \"drive/MyDrive/datasets/audio_ambiental_narracion/XC237810_1 - Soundscape.mp3\"  # Replace with your path\n",
        "manual_label_file = \"drive/MyDrive/datasets/audio_ambiental_narracion/Labels_XC237810_1.txt\"  # Replace with your manual label file path\n",
        "output_with_voice = \"output_with_voice_huggingface.wav\"\n",
        "output_with_silence = \"output_with_silence_huggingface.wav\"\n",
        "vad_output_file = \"Labels_XC237810_1_detection.txt\"  # The file to save the VAD intervals\n",
        "\n",
        "# Check if the input file exists\n",
        "if not os.path.exists(mp3_path):\n",
        "    print(f\"Error: Audio file not found at {mp3_path}\")\n",
        "    exit()\n",
        "\n",
        "# Detect speech intervals\n",
        "speech_intervals, duration = detect_speech_intervals(mp3_path, pipeline)\n",
        "print(f\"VAD Intervals (Pipeline): {speech_intervals}\")\n",
        "\n",
        "# Load manual labels\n",
        "manual_intervals = load_manual_labels(manual_label_file)\n",
        "if manual_intervals is None:\n",
        "    exit()\n",
        "print(f\"Manual Intervals: {manual_intervals}\")\n",
        "\n",
        "# Save the VAD intervals to file, including performance metrics\n",
        "save_vad_intervals(speech_intervals, manual_intervals, vad_output_file)\n",
        "\n",
        "# Load audio\n",
        "audio_segment = load_audio(mp3_path)\n",
        "\n",
        "# Save new audio files with intervals\n",
        "save_audio_with_intervals(audio_segment, speech_intervals, output_with_voice, output_with_silence, duration)\n",
        "print(\"Finished!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torchenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
