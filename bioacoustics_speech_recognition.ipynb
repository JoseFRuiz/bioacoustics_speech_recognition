{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3ypK4ZSF9mz"
      },
      "source": [
        "# WebRTC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vcwn0QnvQg6A",
        "outputId": "af257e65-6965-439b-ee0b-075a1d04f4cf"
      },
      "outputs": [],
      "source": [
        "# !apt-get install ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO1HHryQjwjy",
        "outputId": "4e30d4ce-2ae8-416d-f865-2861660feb31"
      },
      "outputs": [],
      "source": [
        "import webrtcvad\n",
        "import collections\n",
        "import contextlib\n",
        "import wave\n",
        "import os\n",
        "import subprocess\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# Convert MP3 to WAV\n",
        "def convert_mp3_to_wav(mp3_path, wav_path):\n",
        "    audio = AudioSegment.from_mp3(mp3_path)\n",
        "    audio = audio.set_frame_rate(16000)  # Resample to 16000 Hz\n",
        "    audio = audio.set_channels(1)  # Ensure audio is mono\n",
        "    audio.export(wav_path, format=\"wav\")\n",
        "\n",
        "# Frame class to hold audio data\n",
        "class Frame(object):\n",
        "    def __init__(self, bytes, timestamp, duration):\n",
        "        self.bytes = bytes\n",
        "        self.timestamp = timestamp\n",
        "        self.duration = duration\n",
        "\n",
        "def read_wave(path):\n",
        "    with contextlib.closing(wave.open(path, 'rb')) as wf:\n",
        "        num_channels = wf.getnchannels()\n",
        "        assert num_channels == 1\n",
        "        sample_width = wf.getsampwidth()\n",
        "        assert sample_width == 2\n",
        "        sample_rate = wf.getframerate()\n",
        "        assert sample_rate in (8000, 16000, 32000, 48000)\n",
        "        pcm_data = wf.readframes(wf.getnframes())\n",
        "        return pcm_data, sample_rate\n",
        "\n",
        "def frame_generator(frame_duration_ms, audio, sample_rate):\n",
        "    n = int(sample_rate * (frame_duration_ms / 1000.0) * 2)\n",
        "    offset = 0\n",
        "    timestamp = 0.0\n",
        "    duration = (float(n) / sample_rate) / 2.0\n",
        "    while offset + n < len(audio):\n",
        "        yield Frame(audio[offset:offset + n], timestamp, duration)\n",
        "        timestamp += duration\n",
        "        offset += n\n",
        "\n",
        "def vad_collector(sample_rate, frame_duration_ms, padding_duration_ms, vad, frames):\n",
        "    num_padding_frames = int(padding_duration_ms / frame_duration_ms)\n",
        "    ring_buffer = collections.deque(maxlen=num_padding_frames)\n",
        "    triggered = False\n",
        "\n",
        "    voiced_frames = []\n",
        "    intervals = []\n",
        "\n",
        "    for frame in frames:\n",
        "        is_speech = vad.is_speech(frame.bytes, sample_rate)\n",
        "\n",
        "        if not triggered:\n",
        "            ring_buffer.append((frame, is_speech))\n",
        "            num_voiced = len([f for f, speech in ring_buffer if speech])\n",
        "            if num_voiced > 0.9 * ring_buffer.maxlen:\n",
        "                triggered = True\n",
        "                intervals.append((ring_buffer[0][0].timestamp, None))\n",
        "                for f, s in ring_buffer:\n",
        "                    voiced_frames.append(f)\n",
        "                ring_buffer.clear()\n",
        "        else:\n",
        "            voiced_frames.append(frame)\n",
        "            ring_buffer.append((frame, is_speech))\n",
        "            num_unvoiced = len([f for f, speech in ring_buffer if not speech])\n",
        "            if num_unvoiced > 0.9 * ring_buffer.maxlen:\n",
        "                intervals[-1] = (intervals[-1][0], frame.timestamp + frame.duration)\n",
        "                triggered = False\n",
        "                ring_buffer.clear()\n",
        "\n",
        "    if triggered:\n",
        "        intervals[-1] = (intervals[-1][0], frame.timestamp + frame.duration)\n",
        "\n",
        "    return intervals\n",
        "\n",
        "def save_audio_with_intervals(wav_path, intervals, output_with_voice, output_with_silence):\n",
        "    audio = AudioSegment.from_wav(wav_path)\n",
        "    silent_audio = AudioSegment.silent(duration=len(audio))\n",
        "\n",
        "    audio_with_voice = silent_audio\n",
        "    audio_with_silence = audio\n",
        "\n",
        "    for start, end in intervals:\n",
        "        start_ms = int(start * 1000)\n",
        "        end_ms = int(end * 1000)\n",
        "        audio_with_voice = audio_with_voice.overlay(audio[start_ms:end_ms], position=start_ms)\n",
        "        audio_with_silence = audio_with_silence.overlay(silent_audio[start_ms:end_ms], position=start_ms)\n",
        "\n",
        "    audio_with_voice.export(output_with_voice, format=\"wav\")\n",
        "    audio_with_silence.export(output_with_silence, format=\"wav\")\n",
        "\n",
        "# Paths\n",
        "mp3_path = os.path.join(\"data\",\"XC240120 - Soundscape.mp3\")  # Upload your MP3 file to Colab\n",
        "wav_path = \"temp_audio.wav\"\n",
        "output_with_voice = \"output_with_voice_WebRTC.wav\"\n",
        "output_with_silence = \"output_with_silence_WebRTC.wav\"\n",
        "\n",
        "# Convert MP3 to WAV\n",
        "convert_mp3_to_wav(mp3_path, wav_path)\n",
        "\n",
        "# Read WAV file\n",
        "audio, sample_rate = read_wave(wav_path)\n",
        "\n",
        "# Initialize VAD\n",
        "vad = webrtcvad.Vad(3)\n",
        "\n",
        "# Generate frames\n",
        "frames = frame_generator(30, audio, sample_rate)\n",
        "frames = list(frames)\n",
        "\n",
        "# Collect voiced segments\n",
        "intervals = vad_collector(sample_rate, 30, 300, vad, frames)\n",
        "\n",
        "# Print intervals\n",
        "for start, end in intervals:\n",
        "    print(f\"Start: {start:.2f}s, End: {end:.2f}s\")\n",
        "\n",
        "# Save new audio files with intervals\n",
        "save_audio_with_intervals(wav_path, intervals, output_with_voice, output_with_silence)\n",
        "\n",
        "# Cleanup temporary WAV file\n",
        "os.remove(wav_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kk6HOJilGDe7"
      },
      "source": [
        "# Hugging Face Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "pBkwALjQRCJJ",
        "outputId": "9497ce51-0f6c-4208-a98a-dcbe0aff4f84"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyannote.audio import Pipeline\n",
        "from pydub import AudioSegment\n",
        "import numpy as np\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from transformers import pipeline as hf_pipeline\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Read your Hugging Face token from environment. Do NOT hardcode secrets.\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\") or os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
        "if not HF_TOKEN:\n",
        "    raise RuntimeError(\n",
        "        \"Missing Hugging Face token. Set HF_TOKEN or HUGGINGFACE_HUB_TOKEN in your environment.\"\n",
        "    )\n",
        "\n",
        "# Optionally ensure the token is active for the session (no-op if already logged in)\n",
        "try:\n",
        "    login(HF_TOKEN, add_to_git_credential=False)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Model identifier for VAD (pyannote 3.1-compatible)\n",
        "# Use the segmentation model directly instead of the pipeline\n",
        "from pyannote.audio import Model\n",
        "\n",
        "model_name = \"pyannote/segmentation-3.0\"\n",
        "model = Model.from_pretrained(model_name, token=HF_TOKEN)\n",
        "\n",
        "# Create a simple VAD pipeline using the segmentation model\n",
        "from pyannote.audio.pipelines import VoiceActivityDetection\n",
        "\n",
        "pipeline = VoiceActivityDetection(segmentation=model)\n",
        "\n",
        "# Instantiate the pipeline with default parameters\n",
        "HYPER_PARAMETERS = {\n",
        "    \"min_duration_on\": 0.1,\n",
        "    \"min_duration_off\": 0.1,\n",
        "}\n",
        "pipeline.instantiate(HYPER_PARAMETERS)\n",
        "\n",
        "# Function to load and preprocess audio\n",
        "def load_and_preprocess_audio(audio_path):\n",
        "    \"\"\"Loads and preprocesses audio for VAD.  Resamples to 16kHz mono, converts to NumPy array.\"\"\"\n",
        "    try:\n",
        "        audio = AudioSegment.from_file(audio_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading audio file {audio_path}: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "    audio = audio.set_channels(1)  # Convert to mono\n",
        "    audio = audio.set_frame_rate(16000)  # Resample to 16kHz\n",
        "    samples = np.array(audio.get_array_of_samples())\n",
        "    samples = samples.astype(np.float32) / np.iinfo(np.int16).max  # Normalize\n",
        "    return samples, audio, audio.frame_rate\n",
        "\n",
        "# Function to perform VAD\n",
        "def detect_speech_intervals(samples, sample_rate, pipeline):\n",
        "    \"\"\"Performs voice activity detection on the audio.\"\"\"\n",
        "\n",
        "    try:\n",
        "        # The pipeline expects a dictionary with 'waveform' and 'sample_rate'\n",
        "        input_data = {\"waveform\": torch.from_numpy(samples).unsqueeze(0), \"sample_rate\": sample_rate}\n",
        "        vad_result = pipeline(input_data)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during VAD: {e}\")\n",
        "        return [], 0.0\n",
        "\n",
        "    intervals = []\n",
        "    for segment in vad_result.get_timeline():\n",
        "        intervals.append((segment.start, segment.end))\n",
        "    return intervals, 0.0\n",
        "\n",
        "# Function to save audio with intervals\n",
        "def save_audio_with_intervals(audio, intervals, output_with_voice, output_with_silence, duration):\n",
        "    silent_audio = AudioSegment.silent(duration=len(audio))\n",
        "\n",
        "    audio_with_voice = silent_audio\n",
        "    audio_with_silence = audio\n",
        "\n",
        "    for start, end in intervals:\n",
        "        start_ms = int(start * 1000)\n",
        "        end_ms = int(end * 1000)\n",
        "        audio_with_voice = audio_with_voice.overlay(audio[start_ms:end_ms], position=start_ms)\n",
        "        audio_with_silence = audio_with_silence.overlay(silent_audio[start_ms:end_ms], position=start_ms)\n",
        "\n",
        "    audio_with_voice.export(output_with_voice, format=\"wav\")\n",
        "    audio_with_silence.export(output_with_silence, format=\"wav\")\n",
        "\n",
        "# Paths\n",
        "# Update this to a valid local path on your machine\n",
        "mp3_path = os.path.join(\"data\",\"XC240120 - Soundscape.mp3\") \n",
        "\n",
        "# Validate input path early\n",
        "if not os.path.isfile(mp3_path):\n",
        "    raise FileNotFoundError(\n",
        "        f\"Audio file not found: {mp3_path}. Update 'mp3_path' to a valid local file.\"\n",
        "    )\n",
        "output_with_voice = \"output_with_voice_huggingface.wav\"\n",
        "output_with_silence = \"output_with_silence_huggingface.wav\"\n",
        "\n",
        "# Load and preprocess audio\n",
        "samples, audio_segment, sample_rate = load_and_preprocess_audio(mp3_path)\n",
        "\n",
        "# Detect speech intervals\n",
        "speech_intervals, duration = detect_speech_intervals(samples, sample_rate, pipeline)\n",
        "duration = len(audio_segment) / 1000.0\n",
        "\n",
        "print(speech_intervals)\n",
        "\n",
        "# Save new audio files with intervals\n",
        "save_audio_with_intervals(audio_segment, speech_intervals, output_with_voice, output_with_silence, duration)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1L41vL6Q6KC"
      },
      "source": [
        "## Old version performance evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItX6Em9hDCQc"
      },
      "outputs": [],
      "source": [
        "from pyannote.audio import Pipeline\n",
        "from pydub import AudioSegment\n",
        "import numpy as np\n",
        "import os  # For file existence check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KULDOHjkLxMz",
        "outputId": "e70bd69b-38f2-40b2-bb78-28e9edfdf8a6"
      },
      "outputs": [],
      "source": [
        "# Load the pre-trained VAD pipeline\n",
        "try:\n",
        "    pipeline = Pipeline.from_pretrained(\"pyannote/voice-activity-detection\", use_auth_token=True)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading pipeline: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Function to load audio\n",
        "def load_audio(audio_path):\n",
        "    audio = AudioSegment.from_file(audio_path)\n",
        "    return audio\n",
        "\n",
        "# Function to perform VAD\n",
        "def detect_speech_intervals(audio_path, pipeline):\n",
        "    audio = load_audio(audio_path)\n",
        "    duration = len(audio) / 1000.0  # Convert to seconds\n",
        "    vad_result = pipeline({\"uri\": audio_path, \"audio\": audio_path})\n",
        "\n",
        "    intervals = []\n",
        "    for segment in vad_result.get_timeline():\n",
        "        intervals.append((segment.start, segment.end))\n",
        "    return intervals, duration\n",
        "\n",
        "# Function to load manual labels from file\n",
        "def load_manual_labels(label_file):\n",
        "    \"\"\"Loads manual labels from the given text file.\"\"\"\n",
        "    labels = []\n",
        "    try:\n",
        "        with open(label_file, 'r') as f:\n",
        "            for line in f:\n",
        "                start, end, label = line.strip().split('\\t')\n",
        "                labels.append((float(start), float(end)))\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Label file not found: {label_file}\")\n",
        "        return None\n",
        "    except ValueError:\n",
        "        print(f\"Error: Invalid format in label file {label_file}.  Expecting tab-separated start, end, label.\")\n",
        "        return None\n",
        "    return labels\n",
        "\n",
        "# Function to calculate precision and recall\n",
        "def calculate_precision_recall_f1(vad_intervals, manual_intervals):\n",
        "    \"\"\"Calculates precision, recall, and F1-score.\"\"\"\n",
        "\n",
        "    # Convert intervals to sets of time points for easier comparison\n",
        "    def intervals_to_set(intervals):\n",
        "        time_points = set()\n",
        "        for start, end in intervals:\n",
        "            for i in range(int(start * 100), int(end * 100)):  # Convert to 10ms resolution\n",
        "                time_points.add(i)\n",
        "        return time_points\n",
        "\n",
        "    vad_set = intervals_to_set(vad_intervals)\n",
        "    manual_set = intervals_to_set(manual_intervals)\n",
        "\n",
        "    true_positives = len(vad_set.intersection(manual_set))\n",
        "    predicted_positives = len(vad_set)\n",
        "    actual_positives = len(manual_set)\n",
        "\n",
        "    precision = true_positives / predicted_positives if predicted_positives > 0 else 0\n",
        "    recall = true_positives / actual_positives if actual_positives > 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    return precision, recall, f1\n",
        "\n",
        "# Function to save VAD intervals to a file, including performance metrics\n",
        "def save_vad_intervals(vad_intervals, manual_intervals, output_file):\n",
        "    \"\"\"Saves VAD intervals to a text file, including performance metrics.\"\"\"\n",
        "    try:\n",
        "        with open(output_file, 'w') as f:\n",
        "            # Calculate performance metrics\n",
        "            precision, recall, f1 = calculate_precision_recall_f1(vad_intervals, manual_intervals)\n",
        "\n",
        "            # Write header with performance metrics\n",
        "            f.write(f\"# Precision: {precision:.4f}\\n\")\n",
        "            f.write(f\"# Recall: {recall:.4f}\\n\")\n",
        "            f.write(f\"# F1-score: {f1:.4f}\\n\")\n",
        "            f.write(\"# start\\tend\\tVAD\\n\") # Header for intervals\n",
        "\n",
        "            # Write VAD intervals\n",
        "            for start, end in vad_intervals:\n",
        "                f.write(f\"{start}\\t{end}\\tVAD\\n\")\n",
        "\n",
        "        print(f\"VAD intervals and performance metrics saved to {output_file}\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall: {recall:.4f}\")\n",
        "        print(f\"F1-score: {f1:.4f}\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving VAD intervals: {e}\")\n",
        "\n",
        "# Function to save audio with intervals\n",
        "def save_audio_with_intervals(audio, intervals, output_with_voice, output_with_silence, duration):\n",
        "    silent_audio = AudioSegment.silent(duration=len(audio))\n",
        "\n",
        "    audio_with_voice = silent_audio\n",
        "    audio_with_silence = audio\n",
        "\n",
        "    for start, end in intervals:\n",
        "        start_ms = int(start * 1000)\n",
        "        end_ms = int(end * 1000)\n",
        "        audio_with_voice = audio_with_voice.overlay(audio[start_ms:end_ms], position=start_ms)\n",
        "        audio_with_silence = audio_with_silence.overlay(silent_audio[start_ms:end_ms], position=start_ms)\n",
        "\n",
        "    audio_with_voice.export(output_with_voice, format=\"wav\")\n",
        "    audio_with_silence.export(output_with_silence, format=\"wav\")\n",
        "\n",
        "# Paths\n",
        "mp3_path = \"drive/MyDrive/datasets/audio_ambiental_narracion/XC237810_1 - Soundscape.mp3\"  # Replace with your path\n",
        "manual_label_file = \"drive/MyDrive/datasets/audio_ambiental_narracion/Labels_XC237810_1.txt\"  # Replace with your manual label file path\n",
        "output_with_voice = \"output_with_voice_huggingface.wav\"\n",
        "output_with_silence = \"output_with_silence_huggingface.wav\"\n",
        "vad_output_file = \"Labels_XC237810_1_detection.txt\"  # The file to save the VAD intervals\n",
        "\n",
        "# Check if the input file exists\n",
        "if not os.path.exists(mp3_path):\n",
        "    print(f\"Error: Audio file not found at {mp3_path}\")\n",
        "    exit()\n",
        "\n",
        "# Detect speech intervals\n",
        "speech_intervals, duration = detect_speech_intervals(mp3_path, pipeline)\n",
        "print(f\"VAD Intervals (Pipeline): {speech_intervals}\")\n",
        "\n",
        "# Load manual labels\n",
        "manual_intervals = load_manual_labels(manual_label_file)\n",
        "if manual_intervals is None:\n",
        "    exit()\n",
        "print(f\"Manual Intervals: {manual_intervals}\")\n",
        "\n",
        "# Save the VAD intervals to file, including performance metrics\n",
        "save_vad_intervals(speech_intervals, manual_intervals, vad_output_file)\n",
        "\n",
        "# Load audio\n",
        "audio_segment = load_audio(mp3_path)\n",
        "\n",
        "# Save new audio files with intervals\n",
        "save_audio_with_intervals(audio_segment, speech_intervals, output_with_voice, output_with_silence, duration)\n",
        "print(\"Finished!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torchenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
